# modules/vision_enhanced.py
"""
–£–õ–£–ß–®–ï–ù–ù–´–ô VISION –ú–û–î–£–õ–¨ –° –ö–û–°–ú–ò–ß–ï–°–ö–ò–ú AI

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç:
‚Ä¢ –ë–∞–∑–æ–≤—É—é YOLO –¥–µ—Ç–µ–∫—Ü–∏—é (Eyes)
‚Ä¢ Advanced Intelligence Engine
‚Ä¢ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é
‚Ä¢ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä
‚Ä¢ ML-based –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     INPUT FRAME                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                      ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  YOLO  ‚îÇ          ‚îÇ  FEATURES  ‚îÇ
   ‚îÇDetection‚îÇ          ‚îÇ Extraction ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                      ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  Advanced AI Engine  ‚îÇ
       ‚îÇ  ‚Ä¢ 3D Reconstruction ‚îÇ
       ‚îÇ  ‚Ä¢ Prediction        ‚îÇ
       ‚îÇ  ‚Ä¢ Interpolation     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ    OUTPUT MODEL      ‚îÇ
       ‚îÇ  ‚Ä¢ Complete 3D       ‚îÇ
       ‚îÇ  ‚Ä¢ Confidence map    ‚îÇ
       ‚îÇ  ‚Ä¢ AI feedback       ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""

import logging
from typing import Dict, List, Optional, Any, Tuple
import numpy as np
import cv2
from dataclasses import dataclass

try:
    from modules.advanced_intelligence import (
        AdvancedIntelligenceEngine,
        StructuralElement,
        Point3D,
        StructureType,
        ConfidenceLevel
    )
    ADVANCED_AI_AVAILABLE = True
except ImportError:
    ADVANCED_AI_AVAILABLE = False

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO = None
    YOLO_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class EnhancedFeedback:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    quality_score: float  # 0-100
    coverage_percentage: float  # –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
    reconstruction_quality: str  # POOR / FAIR / GOOD / EXCELLENT
    
    # –ü–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    instructions: List[str]
    warnings: List[str]
    suggestions: List[str]
    
    # –°—Ç–∞—Ç—É—Å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
    is_ready: bool  # –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é
    min_points_needed: int
    current_points: int
    
    # –î–µ—Ç–∞–ª–∏ AI –∞–Ω–∞–ª–∏–∑–∞
    detected_elements_count: int
    predicted_elements_count: int
    confidence_avg: float
    
    # –í–∏–∑—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å
    heatmap_data: Optional[Dict[str, Any]] = None
    recommended_scan_areas: List[Dict[str, float]] = None

    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è API"""
        return {
            'quality_score': round(self.quality_score, 1),
            'coverage': round(self.coverage_percentage, 1),
            'reconstruction_quality': self.reconstruction_quality,
            'instructions': self.instructions,
            'warnings': self.warnings,
            'suggestions': self.suggestions,
            'is_ready': self.is_ready,
            'min_points_needed': self.min_points_needed,
            'current_points': self.current_points,
            'detected_count': self.detected_elements_count,
            'predicted_count': self.predicted_elements_count,
            'confidence': round(self.confidence_avg, 2)
        }


class EnhancedVisionSystem:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è —Å AI –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º.
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    ‚Ä¢ YOLO –¥–µ—Ç–µ–∫—Ü–∏—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    ‚Ä¢ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å—Ü–µ–Ω—ã
    ‚Ä¢ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö —á–∞—Å—Ç–µ–π
    ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å
    ‚Ä¢ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    """
    
    def __init__(self, model_path: str = "yolov8n.pt"):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        self.yolo_model = None
        if YOLO_AVAILABLE:
            try:
                self.yolo_model = YOLO(model_path)
                logger.info(f"‚úì YOLO model loaded: {model_path}")
            except Exception as e:
                logger.error(f"Failed to load YOLO: {e}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Advanced AI Engine
        self.ai_engine = None
        if ADVANCED_AI_AVAILABLE:
            try:
                self.ai_engine = AdvancedIntelligenceEngine()
                logger.info("‚úì Advanced AI Engine initialized")
            except Exception as e:
                logger.error(f"Failed to init AI Engine: {e}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞
        self.min_quality_score = 60.0
        self.min_coverage = 50.0
        self.min_points = 2
        
        # –ö–µ—à –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.frame_cache: List[Dict] = []
        self.last_feedback: Optional[EnhancedFeedback] = None
        
        logger.info("‚úì Enhanced Vision System initialized")
    
    def process_frame(
        self,
        image_bytes: bytes,
        camera_pose: np.ndarray,
        ar_points: List[Dict[str, float]]
    ) -> EnhancedFeedback:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ —Å –ø–æ–ª–Ω—ã–º AI –∞–Ω–∞–ª–∏–∑–æ–º.
        
        Args:
            image_bytes: –ë–∞–π—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            camera_pose: 4x4 –º–∞—Ç—Ä–∏—Ü–∞ –∫–∞–º–µ—Ä—ã –∏–ª–∏ 7 –∑–Ω–∞—á–µ–Ω–∏–π (x,y,z,qx,qy,qz,qw)
            ar_points: –¢–æ—á–∫–∏ –æ–ø–æ—Ä—ã –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            EnhancedFeedback —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
        """
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        frame = self._decode_image(image_bytes)
        
        if frame is None:
            return self._create_error_feedback("–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞–¥—Ä–∞")
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ YOLO
        detected_objects = self._detect_objects(frame)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è camera_pose
        pose_matrix = self._convert_pose(camera_pose)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Advanced AI Engine
        if self.ai_engine:
            ai_result = self.ai_engine.process_frame(
                image=frame,
                camera_pose=pose_matrix,
                detected_objects=detected_objects,
                ar_points=ar_points
            )
        else:
            ai_result = {
                'point_cloud_size': 0,
                'detected_elements': len(detected_objects),
                'predicted_elements': 0,
                'coverage_percentage': 0,
                'reconstruction_quality': 'UNAVAILABLE'
            }
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        feedback = self._generate_feedback(
            frame=frame,
            detected_objects=detected_objects,
            ar_points=ar_points,
            ai_result=ai_result
        )
        
        self.last_feedback = feedback
        return feedback
    
    def _detect_objects(self, frame: np.ndarray) -> List[Dict[str, Any]]:
        """
        YOLO –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∫–∞–¥—Ä–µ.
        """
        if self.yolo_model is None:
            return []
        
        try:
            results = self.yolo_model.predict(source=frame, verbose=False)
            
            objects = []
            for result in results:
                boxes = result.boxes
                
                for box in boxes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0])
                    cls = int(box.cls[0])
                    
                    # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–∞
                    class_name = self._map_class_id(cls)
                    
                    objects.append({
                        'type': class_name,
                        'bbox': [float(x1), float(y1), float(x2), float(y2)],
                        'confidence': conf,
                        'center': [(x1 + x2) / 2, (y1 + y2) / 2],
                        'depth': 5.0  # Placeholder, –¥–æ–ª–∂–Ω–æ –∏–¥—Ç–∏ –æ—Ç ARCore
                    })
            
            return objects
            
        except Exception as e:
            logger.error(f"YOLO detection failed: {e}")
            return []
    
    def _generate_feedback(
        self,
        frame: np.ndarray,
        detected_objects: List[Dict],
        ar_points: List[Dict],
        ai_result: Dict[str, Any]
    ) -> EnhancedFeedback:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–º–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        """
        instructions = []
        warnings = []
        suggestions = []
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫
        num_points = len(ar_points)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–¥—Ä–∞
        blur_score = self._assess_blur(frame)
        lighting_score = self._assess_lighting(frame)
        
        # –ö–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        quality_score = self._calculate_quality_score(
            blur_score=blur_score,
            lighting_score=lighting_score,
            num_points=num_points,
            coverage=ai_result.get('coverage_percentage', 0),
            ai_quality=ai_result.get('reconstruction_quality', 'POOR')
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        if num_points < self.min_points:
            instructions.append(
                f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–∏–Ω–∏–º—É–º {self.min_points} —Ç–æ—á–µ–∫ –æ–ø–æ—Ä—ã. "
                f"–°–µ–π—á–∞—Å: {num_points}"
            )
        else:
            instructions.append(
                f"–¢–æ—á–µ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {num_points}. "
                f"–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {quality_score:.0f}%"
            )
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        if blur_score < 0.3:
            warnings.append("‚ö†Ô∏è –ö–∞–¥—Ä —Ä–∞–∑–º—ã—Ç. –î–≤–∏–≥–∞–π—Ç–µ—Å—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ")
        
        if lighting_score < 0.4:
            warnings.append("‚ö†Ô∏è –ü–ª–æ—Ö–æ–µ –æ—Å–≤–µ—â–µ–Ω–∏–µ. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ–Ω–∞—Ä–∏–∫")
        
        coverage = ai_result.get('coverage_percentage', 0)
        if coverage < self.min_coverage:
            warnings.append(
                f"‚ö†Ô∏è –ü–æ–∫—Ä—ã—Ç–∏–µ {coverage:.0f}%. "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è ‚â•{self.min_coverage:.0f}%"
            )
        
        # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        if len(detected_objects) < 3:
            suggestions.append(
                "üí° –ú–∞–ª–æ –≤–∏–¥–∏–º—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ü–æ–¥–æ–π–¥–∏—Ç–µ –±–ª–∏–∂–µ –∫ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"
            )
        
        if num_points >= self.min_points and coverage < 70:
            suggestions.append(
                "üí° –û–±–æ–π–¥–∏—Ç–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é —Å –¥—Ä—É–≥–∏—Ö —Å—Ç–æ—Ä–æ–Ω –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏"
            )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é
        is_ready = (
            num_points >= self.min_points and
            quality_score >= self.min_quality_score and
            coverage >= self.min_coverage
        )
        
        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å AI
        confidence_avg = 0.7  # Placeholder
        if self.ai_engine:
            conf_map = self.ai_engine._generate_confidence_map()
            confidence_avg = conf_map.get('average_confidence', 0.7)
        
        return EnhancedFeedback(
            quality_score=quality_score,
            coverage_percentage=coverage,
            reconstruction_quality=ai_result.get('reconstruction_quality', 'FAIR'),
            instructions=instructions,
            warnings=warnings,
            suggestions=suggestions,
            is_ready=is_ready,
            min_points_needed=self.min_points,
            current_points=num_points,
            detected_elements_count=ai_result.get('detected_elements', 0),
            predicted_elements_count=ai_result.get('predicted_elements', 0),
            confidence_avg=confidence_avg
        )
    
    def get_complete_model(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é 3D –º–æ–¥–µ–ª—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏.
        """
        if self.ai_engine:
            return self.ai_engine.get_complete_model()
        else:
            return {
                'point_cloud': [],
                'structural_elements': [],
                'bounds': None,
                'quality_metrics': {
                    'coverage': 0,
                    'reconstruction_quality': 'UNAVAILABLE',
                    'total_frames': 0
                }
            }
    
    def clear_session(self):
        """–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–Ω–æ–≤–∞—è —Å–µ—Å—Å–∏—è)"""
        if self.ai_engine:
            self.ai_engine.clear()
        
        self.frame_cache.clear()
        self.last_feedback = None
        
        logger.info("Vision system cleared")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def _decode_image(self, image_bytes: bytes) -> Optional[np.ndarray]:
        """–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –±–∞–π—Ç–æ–≤"""
        try:
            nparr = np.frombuffer(image_bytes, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            return frame
        except Exception as e:
            logger.error(f"Image decode error: {e}")
            return None
    
    def _convert_pose(self, camera_pose: Any) -> np.ndarray:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è camera_pose –≤ 4x4 –º–∞—Ç—Ä–∏—Ü—É.
        
        –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
        - 4x4 numpy array
        - 7 –∑–Ω–∞—á–µ–Ω–∏–π (x, y, z, qx, qy, qz, qw)
        - List –∏–∑ 7 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        """
        if isinstance(camera_pose, np.ndarray):
            if camera_pose.shape == (4, 4):
                return camera_pose
            elif camera_pose.shape == (7,):
                return self._pose_from_quaternion(camera_pose)
        
        if isinstance(camera_pose, (list, tuple)):
            if len(camera_pose) == 7:
                return self._pose_from_quaternion(np.array(camera_pose))
        
        # Fallback: identity matrix
        return np.eye(4)
    
    def _pose_from_quaternion(self, pose_7: np.ndarray) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ 4x4 –º–∞—Ç—Ä–∏—Ü—ã –∏–∑ [x,y,z,qx,qy,qz,qw]
        """
        x, y, z, qx, qy, qz, qw = pose_7
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è quaternion –≤ rotation matrix
        # (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å scipy.spatial.transform)
        
        matrix = np.eye(4)
        matrix[0, 3] = x
        matrix[1, 3] = y
        matrix[2, 3] = z
        
        # Rotation part (simplified)
        xx, yy, zz = qx*qx, qy*qy, qz*qz
        xy, xz, yz = qx*qy, qx*qz, qy*qz
        wx, wy, wz = qw*qx, qw*qy, qw*qz
        
        matrix[0, 0] = 1 - 2*(yy + zz)
        matrix[0, 1] = 2*(xy - wz)
        matrix[0, 2] = 2*(xz + wy)
        
        matrix[1, 0] = 2*(xy + wz)
        matrix[1, 1] = 1 - 2*(xx + zz)
        matrix[1, 2] = 2*(yz - wx)
        
        matrix[2, 0] = 2*(xz - wy)
        matrix[2, 1] = 2*(yz + wx)
        matrix[2, 2] = 1 - 2*(xx + yy)
        
        return matrix
    
    def _map_class_id(self, class_id: int) -> str:
        """–ú–∞–ø–ø–∏–Ω–≥ YOLO class ID –Ω–∞ —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ"""
        class_map = {
            0: "beam",
            1: "pipe_obstacle",
            2: "safety_equipment",
            3: "column",
            4: "floor_slab",
            5: "cable_tray"
        }
        return class_map.get(class_id, "unknown")
    
    def _assess_blur(self, frame: np.ndarray) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏ –∫–∞–¥—Ä–∞ (0-1, –≤—ã—à–µ = –ª—É—á—à–µ).
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç variance of Laplacian.
        """
        if frame is None or frame.size == 0:
            return 0.0
        
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # Resize –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            small = cv2.resize(gray, (320, 240))
            
            # Laplacian variance
            laplacian_var = cv2.Laplacian(small, cv2.CV_64F).var()
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ 0-1
            # –û–±—ã—á–Ω–æ —Ö–æ—Ä–æ—à–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–º–µ–µ—Ç variance > 100
            score = min(laplacian_var / 200.0, 1.0)
            
            return score
            
        except Exception as e:
            logger.error(f"Blur assessment failed: {e}")
            return 0.5
    
    def _assess_lighting(self, frame: np.ndarray) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ –æ—Å–≤–µ—â—ë–Ω–Ω–æ—Å—Ç–∏ –∫–∞–¥—Ä–∞ (0-1).
        """
        if frame is None or frame.size == 0:
            return 0.0
        
        try:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # –°—Ä–µ–¥–Ω—è—è —è—Ä–∫–æ—Å—Ç—å
            mean_brightness = np.mean(gray)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ 80-180
            if mean_brightness < 80:
                score = mean_brightness / 80.0
            elif mean_brightness > 180:
                score = (255 - mean_brightness) / 75.0
            else:
                score = 1.0
            
            return max(0.0, min(1.0, score))
            
        except Exception as e:
            logger.error(f"Lighting assessment failed: {e}")
            return 0.5
    
    def _calculate_quality_score(
        self,
        blur_score: float,
        lighting_score: float,
        num_points: int,
        coverage: float,
        ai_quality: str
    ) -> float:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ (0-100).
        """
        # –í–µ—Å–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        weights = {
            'blur': 0.15,
            'lighting': 0.15,
            'points': 0.25,
            'coverage': 0.30,
            'ai_quality': 0.15
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—á–µ–∫ (2-10)
        points_norm = min(num_points / 10.0, 1.0)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è coverage (0-100%)
        coverage_norm = coverage / 100.0
        
        # AI quality: POOR=0.3, FAIR=0.6, GOOD=0.8, EXCELLENT=1.0
        ai_quality_map = {
            'POOR': 0.3,
            'FAIR': 0.6,
            'GOOD': 0.8,
            'EXCELLENT': 1.0,
            'UNAVAILABLE': 0.5
        }
        ai_quality_norm = ai_quality_map.get(ai_quality, 0.5)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫–æ—Ä
        score = (
            weights['blur'] * blur_score +
            weights['lighting'] * lighting_score +
            weights['points'] * points_norm +
            weights['coverage'] * coverage_norm +
            weights['ai_quality'] * ai_quality_norm
        ) * 100
        
        return round(score, 1)
    
    def _create_error_feedback(self, error_message: str) -> EnhancedFeedback:
        """–°–æ–∑–¥–∞–Ω–∏–µ feedback –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return EnhancedFeedback(
            quality_score=0.0,
            coverage_percentage=0.0,
            reconstruction_quality='ERROR',
            instructions=[],
            warnings=[f"‚ùå {error_message}"],
            suggestions=[],
            is_ready=False,
            min_points_needed=self.min_points,
            current_points=0,
            detected_elements_count=0,
            predicted_elements_count=0,
            confidence_avg=0.0
        )


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ü–£–ë–õ–ò–ß–ù–´–ô API
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_vision_system(model_path: str = "yolov8n.pt") -> EnhancedVisionSystem:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è vision system"""
    return EnhancedVisionSystem(model_path=model_path)


# –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º API
class VisionSystem:
    """
    –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
    """
    def __init__(self):
        self.enhanced = create_vision_system()
    
    def process_scene(
        self,
        image_bytes: bytes,
        pose_matrix: Any,
        markers: List[Dict]
    ) -> Dict[str, Any]:
        """–°—Ç–∞—Ä—ã–π API endpoint"""
        feedback = self.enhanced.process_frame(
            image_bytes=image_bytes,
            camera_pose=pose_matrix,
            ar_points=markers
        )
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
        return feedback.to_dict()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # –¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã
    vision = create_vision_system()
    logger.info("‚úì Enhanced Vision System initialized successfully")
    
    # –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    dummy_image = np.zeros((720, 1280, 3), dtype=np.uint8)
    _, image_bytes = cv2.imencode('.jpg', dummy_image)
    
    feedback = vision.process_frame(
        image_bytes=image_bytes.tobytes(),
        camera_pose=np.eye(4),
        ar_points=[]
    )
    
    logger.info(f"Test feedback: Quality={feedback.quality_score}%")
    logger.info(f"Instructions: {feedback.instructions}")